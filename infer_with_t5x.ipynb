{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 16:20:43.377064: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2023-04-16 16:20:44.005930: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2023-04-16 16:20:44.006035: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2023-04-16 16:20:44.006045: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import os\n",
    "\n",
    "import clu.data.dataset_iterator\n",
    "import tensorflow as tf\n",
    "import jax\n",
    "from jax import random\n",
    "from jax.experimental import multihost_utils\n",
    "import jax.numpy as jnp\n",
    "from flax import linen\n",
    "import numpy as np\n",
    "import seqio\n",
    "import t5.data\n",
    "from t5.evaluation import metrics as t5_metrics\n",
    "\n",
    "import t5x\n",
    "from t5x import partitioning\n",
    "from t5x import train_state as train_state_lib\n",
    "from t5x import utils\n",
    "from t5x.examples.decoder_only import network as decoder_only_network\n",
    "from t5x.examples.t5 import network\n",
    "from t5x.examples.scalable_t5 import network as scalable_network\n",
    "from t5x.interactive_model import InteractiveModel\n",
    "from t5x.interactive_model import get_batches_from_seqio\n",
    "from t5x.interactive_model import InferenceType\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocabulary=seqio.SentencePieceVocabulary(sentencepiece_model_file=\"gs://n2formal-public-data-europe/albert/tokenizer/galactica_enhanced.model\")\n",
    "output_vocabulary=seqio.SentencePieceVocabulary(sentencepiece_model_file=\"gs://n2formal-public-data-europe/albert/tokenizer/galactica_enhanced.model\")\n",
    "optimizer=t5x.adafactor.Adafactor(decay_rate=0.8, step_offset=0, logical_factor_rules=t5x.adafactor.standard_logical_factor_rules())\n",
    "decode_fn=functools.partial(t5x.decoding.temperature_sample, temperature=1.0, topk=40)\n",
    "\n",
    "# Define a model using the minimal T5 module.\n",
    "t5_module = decoder_only_network.Decoder(\n",
    "    config=decoder_only_network.TransformerConfig(\n",
    "        vocab_size=32128,\n",
    "        dtype='bfloat16',\n",
    "        emb_dim=768,\n",
    "        num_heads=12,\n",
    "        num_layers=12,\n",
    "        head_dim=64,\n",
    "        mlp_dim=2048,\n",
    "        mlp_activations=('gelu', 'linear'),\n",
    "        dropout_rate=0.0,\n",
    "        logits_via_embedding=True\n",
    "    )\n",
    ")\n",
    "model = t5x.models.DecoderOnlyModel(\n",
    "    module=t5_module,\n",
    "    vocabulary=input_vocabulary,\n",
    "    optimizer_def=optimizer,\n",
    "    decode_fn=decode_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define EncoderDecoderModel constructor args (except the module).\n",
    "# input_vocabulary=t5.data.get_default_vocabulary()\n",
    "# output_vocabulary=t5.data.get_default_vocabulary()\n",
    "# optimizer=t5x.adafactor.Adafactor(decay_rate=0.8, step_offset=0, logical_factor_rules=t5x.adafactor.standard_logical_factor_rules())\n",
    "# decode_fn=functools.partial(t5x.decoding.temperature_sample, temperature=1.0, topk=40)\n",
    "\n",
    "# # Define a model using the minimal T5 module.\n",
    "# t5_module = network.Transformer(config=network.T5Config(\n",
    "#     vocab_size=32128,\n",
    "#     dtype='bfloat16',\n",
    "#     emb_dim=512,\n",
    "#     num_heads=6,\n",
    "#     num_encoder_layers=8,\n",
    "#     num_decoder_layers=8,\n",
    "#     head_dim=64,\n",
    "#     mlp_dim=1024,\n",
    "#     mlp_activations=('gelu', 'linear'),\n",
    "#     dropout_rate=0.0,\n",
    "#     logits_via_embedding=False))\n",
    "# model = t5x.models.EncoderDecoderModel(\n",
    "#     module=t5_module,\n",
    "#     input_vocabulary=input_vocabulary,\n",
    "#     output_vocabulary=output_vocabulary,\n",
    "#     optimizer_def=optimizer,\n",
    "#     decode_fn=decode_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The checkpoint below is a T5-1.1-Small checkpoint (https://github.com/google-research/t5x/blob/main/docs/models.md) \n",
    "# that has additionally been finetuned on the (Open Domain) Natural Questions \n",
    "# benchmark (https://ai.google.com/research/NaturalQuestions).\n",
    "# checkpoint_path='gs://t5-data/pretrained_models/cbqa/small_ssm_nq/model.ckpt-1110000'\n",
    "checkpoint_path='gs://n2formal-public-data-europe/albert/model_checkpoints/pretrain_lovelace_decoder_base_default/checkpoint_90000/checkpoint'\n",
    "dtype='bfloat16'\n",
    "\n",
    "restore_mode='specific'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioner=partitioning.PjitPartitioner(\n",
    "        num_partitions=1,\n",
    "        model_parallel_submesh=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Since rank of parameter decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_0/relpos_bias/rel_embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_1/relpos_bias/rel_embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_10/relpos_bias/rel_embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_11/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_11/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_11/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_11/relpos_bias/rel_embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_2/relpos_bias/rel_embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_3/relpos_bias/rel_embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_4/relpos_bias/rel_embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_5/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_5/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_5/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_5/relpos_bias/rel_embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_6/relpos_bias/rel_embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_7/relpos_bias/rel_embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_8/relpos_bias/rel_embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_9/relpos_bias/rel_embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "WARNING:absl:Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "WARNING:absl:Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['state', 'target'])\n",
      "dict_keys(['param_states', 'step'])\n",
      "dict_keys(['decoder'])\n",
      "Hooray!\n",
      "dict_keys(['decoder_norm', 'layers_0', 'layers_1', 'layers_10', 'layers_11', 'layers_2', 'layers_3', 'layers_4', 'layers_5', 'layers_6', 'layers_7', 'layers_8', 'layers_9', 'token_embedder'])\n",
      "dict_keys(['decoder_norm', 'layers_0', 'layers_1', 'layers_10', 'layers_11', 'layers_2', 'layers_3', 'layers_4', 'layers_5', 'layers_6', 'layers_7', 'layers_8', 'layers_9', 'token_embedder'])\n",
      "Hooray!\n",
      "dict_keys(['decoder_norm', 'layers_0', 'layers_1', 'layers_10', 'layers_11', 'layers_2', 'layers_3', 'layers_4', 'layers_5', 'layers_6', 'layers_7', 'layers_8', 'layers_9', 'token_embedder'])\n",
      "dict_keys(['decoder_norm', 'layers_0', 'layers_1', 'layers_10', 'layers_11', 'layers_2', 'layers_3', 'layers_4', 'layers_5', 'layers_6', 'layers_7', 'layers_8', 'layers_9', 'token_embedder'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1681662057.014130  887136 gcs_resource.cc:102] Using default AdmissionQueue with limit 32\n",
      "I0000 00:00:1681662057.017613  888218 google_auth_provider.cc:179] Running on GCE, using service account 398558348766-compute@developer.gserviceaccount.com\n",
      "2023-04-16 16:21:05.364077: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2023-04-16 16:21:05.364120: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "batch_size=8\n",
    "task_feature_lengths = {'inputs': 256, 'targets': 256}\n",
    "output_dir='/tmp/output_dir'\n",
    "input_shapes = {\n",
    "    'encoder_input_tokens': np.array([8, 256]),\n",
    "    'decoder_target_tokens': np.array([8, 256]),\n",
    "    'decoder_input_tokens': np.array([8, 256]),\n",
    "    'decoder_loss_weights': np.array([8, 256])\n",
    "}\n",
    "\n",
    "interactive_model = InteractiveModel(\n",
    "  batch_size=batch_size,\n",
    "  task_feature_lengths=task_feature_lengths,\n",
    "  output_dir=output_dir,\n",
    "  partitioner=partitioner,\n",
    "  model=model,\n",
    "  dtype=dtype,\n",
    "  restore_mode=restore_mode,\n",
    "  checkpoint_path=checkpoint_path,\n",
    "  input_shapes=input_shapes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'what is the capital of france?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_vocabulary.encode(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_examples = [\n",
    "  {\n",
    "      'target': 'Paris', \n",
    "      'input': \"Where is the capitcal of France?\"\n",
    "      },\n",
    "  {\n",
    "      'target': 'Rome', \n",
    "      'input': \"Where is the capitcal of France?\"\n",
    "      },\n",
    "  {\n",
    "      'target': 'Moscow', \n",
    "      'input': \"Where is the capitcal of France?\"\n",
    "      },\n",
    "  {\n",
    "      'target': 'London', \n",
    "      'input': \"Where is the capitcal of France?\"\n",
    "      },\n",
    "  {\n",
    "      'target': 'Beijing', \n",
    "      'input': \"Where is the capitcal of France?\"\n",
    "      },\n",
    "  {\n",
    "      'target': 'Tokyo', \n",
    "      'input': \"Where is the capitcal of France?\"\n",
    "      },\n",
    "  {\n",
    "      'target': 'Jakarta', \n",
    "      'input': \"Where is the capitcal of France?\"\n",
    "      },\n",
    "  {\n",
    "      'target': 'Liverpool', \n",
    "      'input': \"lemma \\\"1+2=3\\\" by auto\"\n",
    "      },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/qj213/.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/qj213/.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Where is the capitcal of France?\n",
      "Prediction: Sir Avraham Lehdet ghet, or 'L', or 'F'? That we s'il or two would be best used as a stand on 'T'. You will find that a certain note _t.it_ would come from the house in the centre of England while we could find it. 'T's what ?' 'T's the place d'un petit d'un air-star haste scythes. The room you go into is the room, its apricots which the room will be called: 'T, the house!' 'T!?' 'T!' 'T!' The rooms are a bit smaller to make things work. A few dot scythes and scythes, scythes and one is cyth scythe! The cyth is cyth but all cyth is the place. There is a small cyth scythe. An excellent d'fore it will be to have a large cyth in it. When you are ready to go, look, and look, try the next door. You can scythe _?_ It will come later, a little further. 'T!-----------------------------------------------------------------------------' p. 92. 'L', which ?? 's been snubbed by the lady of the court, is so much so that '-----------------------------------------------------' p. 93. 'Gay well-bred a day, ' \n",
      "Target: Paris\n",
      "Score: -18.63170623779297\n",
      "Input: Where is the capitcal of France?\n",
      "Prediction: y, the French as it should follow. The question is whether or not France was a French, or France had its place in this situation. This case seems to be an open question at the time, or a very open question. But in this case the situation is a very open question and the problem is very different. At the same time, it might seem plausible to suppose that in the 16th century Paris he did what did not. ### _Splash_ What does it mean for the French or English to have the capability to be part of French? The question remains very open as it has been disputed. For French French students we could not imagine that French innumerable French or English as it is a French or English French (or French) and might have the power to do an operation that will succeed under the conditions of its \"abundance system.\" We are admonished to say, \"Before ours — \" the French or English is an English French (or French) or English French — but French or English can be used without a method. As an example the French or English are the French or English French and English, the French and English must be considered as the French or French. The French or English French or English may be used in German or English as \"a king\", but not in English or English. The French or in German was a French or English French arithmetic of Italian descent for purely French or English. The French was born in 1618, and for a second education on French as the French or English French are the English French or English arithmetic. This version, in French or English, was one of Germany's most famous men—but perhaps the English, in German or English, is the most famous. But the French or English can be used as a king at least. For English French French students — French or English ( _ac_ ) is an English or English French (or French) or German (or French) romaine. This version of French French French can, in English, ( _ai_ ), can and is accompanied by French works. In the French and English French schooled French French students ( _\n",
      "Target: Rome\n",
      "Score: -20.760211944580078\n",
      "Input: Where is the capitcal of France?\n",
      "Prediction: (the captivity of England), or the captivity of France, should be used on the basis of what would be the yearly scale of the country. The captor of Italy is a purely theatrical setting, with a different tense and different \"invasion of the country\". This, in that, is a very (if indeed) time, (thus) \"asks of France a 300-year-old and a French 150-year-old isol, and if his \"a\" should return to \"the\", it should return to \"we\". (The stage is played a short while so we may ask ourselves again, as far as we can explain; he is \"a\" for his \"a\".) This is an homage to Italian, and in this \"the\" is the same for England. At the same time, Italy a 400-year-old isol 400-year-old (though we don't ask why). The 400-year-old 000 is a 990-year-old with an old age. I can tell from this that you are a liar and a \"a\" of 400-year-olds with a 300-year-old in the year of France. But by the time of us, Italy, Italy is always very well-defined and very far from being able to count their times. Thus, we must (to) count a 400-year-old with an old age. (In English, there seem to be some \"eighth\") a 220-year-old for a 400-year-old. And yet, we can tell, Italy's Italy is the most well-defined and quite well-defined, and \"and\" is \"the most well-defined and very reasonably well-defined; you may, as a consequence, be well-defined, and it is very far from being able to count, which is very likely the best-place in England.\" ax is the \"\n",
      "Target: Moscow\n",
      "Score: -19.30158805847168\n",
      "Input: Where is the capitcal of France?\n",
      "Prediction: ? Where is a capitcal of Europe? Where is a capitcal of Spain? Where is a capitcal of France? Where is an image of England? Where is a capitcal of the sea? Where is a _(tfyr)_ or _northwards_? Where is a _(tfyr)_ or _everywhere_? Where is a _(tfyr)_ or sea? Where is an image of England? Where is a _(tfyr)_ or sea? Where is Antarctica? Where is Antarctica? Where is a beach in England? Where is a swimming wave? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where has an image of England? Where is Antarctica? Where is Antarctica? Where is Antarctica? And is Antarctica? Where is Arctic Ocean? Where was Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is the United Kingdom, Australia, Australia or Mexico. Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where is Antarctica? Where\n",
      "Target: London\n",
      "Score: -20.1217041015625\n",
      "Input: Where is the capitcal of France?\n",
      "Prediction: l: The glimmer of ... As far as you know, France has been far more difficult than in ... But there is no ... The .... Capitulations are not particularly important for trident. It may be that the French .... Is ... As ... This is in no way too easy. But there is a neednon to look beyond the Capitabilities, not just from the French ... But it is now very difficult. And there are no trident, not quite clear in the .... The French .... ... And the French ... It is always difficult. And there's nothing to see now. I haven't yet seen ... , that's not what is trident; but I'm not sure whether it be possible, for the .... I wouldn't be far from happy; I will probably have something to do with. But I'm not sure whether they are much more or less as they go into that ... That's why I didn't. It's very hard, isn't it? It's only if the .... ... , that in itself, isn't it? It looks exactly as I'm sure you'll have. Or perhaps it seems as a bit swish, that is very much in my view. Is it ... Yes; it's that part of the ... , that part of it, was that part of it? It's frightening! If you don't know ... And you know what I mean! ... It's frightening! And as you've said, in a little little thing, I don't even know a moment because it's an ex-Camb ... And you'd just go to something. That's no way of knowing what I want. I know just what I want. But I wouldn't be a bit swish, not .... I'd really like any oppler\n",
      "Target: Beijing\n",
      "Score: -21.27166748046875\n",
      "Input: Where is the capitcal of France?\n",
      "Prediction: _Pisan_. 32: 1) = 1)/. 1/. 1/. 0/0 ############################ ######## ROOT START #################################### ####### ROOT END ################################ if(nextPath+1==nextPath and(prependPath-1==nextPath)): # = 1) = 1) = 1) = 1) = 1/ (n nextPath+1/. 1/. 0) = 1/. 1/0. #-1/1 #-1/2 #-1/3 #-1/4 #-1/0 #-2/1 ####-- ###### MOVE BETWEEN 2 and 3: <ms>(1,M)=m ############################ ######## ROOT END ########################## ###### ROOT END ############################ ###### END ROOT END ############################## ###### MOVE END #############\n",
      "Target: Tokyo\n",
      "Score: -21.510143280029297\n",
      "Input: Where is the capitcal of France?\n",
      "Prediction: The adage is a denominator. The denominator is similar to that of the denominator, but the second in the fifth. It is, therefore, the denominator that is in order in our sense. Is the denominator a dilhouette? Or is the divisorial del turbulence? The last and most striking of the denominator consists of the sand and the turbulence averaging, but these asymmetric averaging will be observed by Ising and Fuffindin. The first thing is the fourth term that is in terms of the turbulence of the denominator and the one with the denominator (and consequently it is the turbulence of the denominator). This way, you should have a different effect of the adage. There are other features of this approach for which there are multiple sands. There is thus two reasons for this, because for a given denominator it is more common for a given denominator to be a dilhouette (which is exactly opposite the denominator) than the denominator. That is, a given denominator is almost the same and it is, therefore, to our senses. This is in order if the adage is in a different direction than what is shown with a turbulence. To our advantage, this does not mean that the asymmetric averaging might not be the same. Because the asymmetric averaging does do it without any sand, it is not the asymmetric averaging over any turbulence. Here we describe the main point of note regarding this as an asymmetric averaging (in this case, the turbulence of the denominator). It is not known whether this approach can be used as a proof of the necessity of the first and last terms in the proof. To this end, just as a dilhouette is defined above, but I think it requires more attention, since the asymmetric is not more equating the turbulence to a turbulence. Suppose we show that\n",
      "Target: Jakarta\n",
      "Score: -24.842269897460938\n",
      "Input: lemma \"1+2=3\" by auto\n",
      "Prediction: . , 37. @be = 5 * lemmas(1-3x) * sum(x) * p + sum(y)*(x) * (1 - 1/4)(x) - 1, x.r, (p, 1) .r(1.000) >>> sum(x) / (1 + x)(1/4)(x) + 1.22438472592912954e+1632 + 1.868162026064817627e+426 + 1.020662867183351197e+148 + 1.224919656268578941e+624 + 9.33551055736811387f+120 * p, y.r, ((1 - x)**4**2) >>> m = 1e-6 * sum(y) * p + 1e-6 * x + 1/4 2899 7. + 28849545163962 + 2.83405504983738565e+121 + 1.00000000000000000e+09 * p + m (x,p,(1/6 - 1/6 - 1/6 + x)**4**2) - 1.67238461704758 + (1-x +**2/8)**2 + 1.8089433837 + 1.842\n",
      "Target: Liverpool\n",
      "Score: -24.891605377197266\n"
     ]
    }
   ],
   "source": [
    "examples_and_predictions, _ = interactive_model.predict_with_aux(examples=validation_examples)\n",
    "predictions = [prediction for example, prediction in examples_and_predictions]\n",
    "\n",
    "examples_and_scores = interactive_model.score(examples=validation_examples)\n",
    "scores = [score for example, score in examples_and_scores]\n",
    "for val_example, prediction, score in zip(validation_examples, predictions, scores):\n",
    "    print(f\"Input: {val_example['input']}\")\n",
    "    print(f\"Prediction: {prediction.decode('utf-8')}\")\n",
    "    print(f\"Target: {val_example['target']}\")\n",
    "    print(f\"Score: {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
